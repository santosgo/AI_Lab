{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT Research / Creating BERT classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load some pre-trained models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Link to BERT's github containing pre-trained models for BERT: https://github.com/google-research/bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_pretrained_bert import BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The pre-trained model you are loading is a cased model but you have not set `do_lower_case` to False. We are setting `do_lower_case=False` for you but you may want to check this behavior.\n"
     ]
    }
   ],
   "source": [
    "tokenizerM = BertTokenizer.from_pretrained('bert-base-multilingual-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizerP = BertTokenizer.from_pretrained('bert-base-portuguese-cased')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Inspect vocabularies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'TextFiles\\\\vocab.txt'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.save_vocabulary('TextFiles')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30522"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6613"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.vocab.get('brooklyn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['we',\n",
       " \"'\",\n",
       " 're',\n",
       " 'going',\n",
       " 'to',\n",
       " 'brooklyn',\n",
       " 'on',\n",
       " 'october',\n",
       " '27',\n",
       " 'to',\n",
       " 'visit',\n",
       " 'microsoft',\n",
       " 'in',\n",
       " 'delta',\n",
       " 'airlines',\n",
       " 'flight',\n",
       " 'de',\n",
       " '##34',\n",
       " '##52',\n",
       " '.']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(u\"We're going to Brooklyn on October 27 to visit Microsoft in Delta Airlines flight DE3452.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'TextFiles\\\\vocab.txt'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizerM.save_vocabulary('TextFiles')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "119547"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizerM.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'TextFiles\\\\vocab.txt'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizerP.save_vocabulary('TextFiles')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O meu nome existe!\n"
     ]
    }
   ],
   "source": [
    "for token in tokenizerP.vocab.keys():\n",
    "    if token == 'Gon√ßalo':\n",
    "        print('O meu nome existe!')\n",
    "        break\n",
    "else:\n",
    "    print('Tough luck...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2995"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizerP.vocab.get('Santos')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Creating a classifier with Bert"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Making use of the GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(tf.test.gpu_device_name())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'GeForce 940MX'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(torch.cuda.is_available())\n",
    "torch.cuda.get_device_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Installing the Hugging Face library Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Downloading CoLA  dataset\n",
    "COLA stands for Corpus of Linguistic Acceptability\n",
    "more details in https://nyu-mll.github.io/CoLA/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install wget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Donwloading the CoLA dataset...\n",
      "File already downloaded\n"
     ]
    }
   ],
   "source": [
    "import wget\n",
    "import os\n",
    "\n",
    "print('Donwloading the CoLA dataset...')\n",
    "\n",
    "url = 'https://nyu-mll.github.io/CoLA/cola_public_1.1.zip'\n",
    "\n",
    "if not os.path.exists('./cola_public_1.1.zip'):\n",
    "    wget.download(url,'./cola_public_1.1.zip')\n",
    "else:\n",
    "    print('File already downloaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already unzipped\n"
     ]
    }
   ],
   "source": [
    "# Unzip dataset\n",
    "import zipfile\n",
    "if not os.path.exists('./cola_public/'):\n",
    "    with zipfile.ZipFile('./cola_public_1.1.zip', 'r') as zip_f:\n",
    "        zip_f.extractall()\n",
    "else:\n",
    "    print('File already unzipped')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accessing the CoLA data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('./cola_public/raw/in_domain_train.tsv', sep='\\t', header=None, names=['sentence_source','label','label_notes','sentence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_source</th>\n",
       "      <th>label</th>\n",
       "      <th>label_notes</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6923</th>\n",
       "      <td>m_02</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Alan made the loaf with strong white flour.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5496</th>\n",
       "      <td>b_73</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Hilda is such a scholar as you were speaking o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5234</th>\n",
       "      <td>kl93</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>He bought a Honda.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3321</th>\n",
       "      <td>l-93</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I herded the cattle.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8049</th>\n",
       "      <td>ad03</td>\n",
       "      <td>0</td>\n",
       "      <td>*</td>\n",
       "      <td>Aphrodite quickly may free the animals</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7084</th>\n",
       "      <td>sgww85</td>\n",
       "      <td>0</td>\n",
       "      <td>*</td>\n",
       "      <td>The students and Professor Swansong is meeting...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6123</th>\n",
       "      <td>c_13</td>\n",
       "      <td>0</td>\n",
       "      <td>*</td>\n",
       "      <td>She was kissed him.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6889</th>\n",
       "      <td>m_02</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The baby is heavy.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4038</th>\n",
       "      <td>ks08</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Joe warned the class that the exam would be di...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4425</th>\n",
       "      <td>ks08</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>John is running to the car.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     sentence_source  label label_notes  \\\n",
       "6923            m_02      1         NaN   \n",
       "5496            b_73      1         NaN   \n",
       "5234            kl93      1         NaN   \n",
       "3321            l-93      1         NaN   \n",
       "8049            ad03      0           *   \n",
       "7084          sgww85      0           *   \n",
       "6123            c_13      0           *   \n",
       "6889            m_02      1         NaN   \n",
       "4038            ks08      1         NaN   \n",
       "4425            ks08      1         NaN   \n",
       "\n",
       "                                               sentence  \n",
       "6923        Alan made the loaf with strong white flour.  \n",
       "5496  Hilda is such a scholar as you were speaking o...  \n",
       "5234                                 He bought a Honda.  \n",
       "3321                               I herded the cattle.  \n",
       "8049             Aphrodite quickly may free the animals  \n",
       "7084  The students and Professor Swansong is meeting...  \n",
       "6123                                She was kissed him.  \n",
       "6889                                 The baby is heavy.  \n",
       "4038  Joe warned the class that the exam would be di...  \n",
       "4425                        John is running to the car.  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pass sentences and labels to numpy arrays\n",
    "sentences = df.sentence.values\n",
    "labels = df.label.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenization and input formatting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tokenizer.encode function will perform 3 functions which are necessary for the BERT classifier:\n",
    "<br>&nbsp;&nbsp;1) split each sentence into tokens\n",
    "<br>&nbsp;&nbsp;2) add [CLS] and [SEC] tokens to the beginning and end of each sentence respectively\n",
    "<br>&nbsp;&nbsp;3) map the tokens to their IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import Bert tokenizer from the Hugging Face library transformers, not from Pytorch as in the top of this notebook\n",
    "from transformers import BertTokenizer as T_BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "T_tokenizer = T_BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: Our friends won't buy this analysis, let alone the next one we propose.\n",
      "Tokenized: ['our', 'friends', 'won', \"'\", 't', 'buy', 'this', 'analysis', ',', 'let', 'alone', 'the', 'next', 'one', 'we', 'propose', '.']\n",
      "Encoded: [101, 2256, 2814, 2180, 1005, 1056, 4965, 2023, 4106, 1010, 2292, 2894, 1996, 2279, 2028, 2057, 16599, 1012, 102]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Original: {sentences[0]}\")\n",
    "print(f\"Tokenized: {T_tokenizer.tokenize(sentences[0])}\")\n",
    "print(f\"Encoded: {T_tokenizer.encode(sentences[0])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids=[]\n",
    "for sent in sentences:\n",
    "    encoded_sent = T_tokenizer.encode(sent,add_special_tokens=True,max_length=64,pad_to_max_length=True)\n",
    "    input_ids.append(encoded_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating attention masks - they have 1 for each word and 0 for padding\n",
    "attention_masks = []\n",
    "for sent in input_ids:\n",
    "    att_mask = [int(token_id > 0) for token_id in sent]\n",
    "    attention_masks.append(att_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating training and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids,labels,random_state=2018,test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_masks, validation_masks, _, _ = train_test_split(attention_masks,labels,random_state=2018,test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert into torch tensors\n",
    "train_inputs = torch.tensor(train_inputs)\n",
    "validation_inputs = torch.tensor(validation_inputs)\n",
    "train_labels = torch.tensor(train_labels)\n",
    "validation_labels = torch.tensor(validation_labels)\n",
    "train_masks = torch.tensor(train_masks)\n",
    "validation_masks = torch.tensor(validation_masks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting up DataLoader from Pytorch to train in batches for better memory use (in this case not needed, so just for future reference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "batch_size = 16\n",
    "\n",
    "train_data = TensorDataset(train_inputs,train_masks,train_labels)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "validation_data = TensorDataset(validation_inputs,validation_masks,validation_labels)\n",
    "validation_sampler = SequentialSampler(validation_data)\n",
    "validation_dataloader = DataLoader(validation_data,sampler=validation_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training the sequence classification model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertForSequenceClassification, AdamW, BertConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = BertForSequenceClassification.from_pretrained('bert-base-uncased',\n",
    "                                                           num_labels=2,\n",
    "                                                           output_attentions=False,\n",
    "                                                           output_hidden_states=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.cuda() # tell pytorch to run this model in GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating optimizer and learning rate scheduler\n",
    "# Optimizer updates the weights:\n",
    "optimizer = AdamW(classifier.parameters(),\n",
    "                 lr = 2e-5, \n",
    "                 eps= 1e-8)\n",
    "#Scheduler handles the learning rate decay:\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "epochs = 4\n",
    "total_steps = len(train_dataloader)*epochs\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Helper function for calculating accuracy\n",
    "import numpy as np\n",
    "def flat_accuracy(preds,labels):\n",
    "    pred_flat = np.argmax(preds,axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat)/(len(labels_flat))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Helper function for formatting elapsed times\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "def format_time(elapsed):\n",
    "    elapsed_rounded = int(round(elapsed))\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actual Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set random values to get reproducibility\n",
    "import random\n",
    "seed_val = 42\n",
    "\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "###### Epoch 1/4 ########\n",
      "Training\n",
      "   Batch 40 of 481. Elapsed: 0:01:45   \n",
      "   Batch 80 of 481. Elapsed: 0:03:30   \n",
      "   Batch 120 of 481. Elapsed: 0:05:14   \n",
      "   Batch 160 of 481. Elapsed: 0:06:59   \n",
      "   Batch 200 of 481. Elapsed: 0:08:44   \n",
      "   Batch 240 of 481. Elapsed: 0:10:28   \n",
      "   Batch 280 of 481. Elapsed: 0:12:13   \n",
      "   Batch 320 of 481. Elapsed: 0:13:58   \n",
      "   Batch 360 of 481. Elapsed: 0:15:42   \n",
      "   Batch 400 of 481. Elapsed: 0:17:27   \n",
      "   Batch 440 of 481. Elapsed: 0:19:11   \n",
      "   Batch 480 of 481. Elapsed: 0:20:56   \n",
      "\n",
      "   Average training loss: 0.4912922078755194\n",
      "   Training epoch took: 1258.6191229820251\n",
      "\n",
      "Running validation...\n",
      "Accuracy: 0.8032407407407407\n",
      "Validation time: 31.767547369003296\n",
      "\n",
      "###### Epoch 2/4 ########\n",
      "Training\n",
      "   Batch 40 of 481. Elapsed: 0:01:44   \n",
      "   Batch 80 of 481. Elapsed: 0:03:29   \n",
      "   Batch 120 of 481. Elapsed: 0:05:14   \n",
      "   Batch 160 of 481. Elapsed: 0:06:58   \n",
      "   Batch 200 of 481. Elapsed: 0:08:43   \n",
      "   Batch 240 of 481. Elapsed: 0:10:28   \n",
      "   Batch 280 of 481. Elapsed: 0:12:13   \n",
      "   Batch 320 of 481. Elapsed: 0:13:58   \n",
      "   Batch 360 of 481. Elapsed: 0:15:43   \n",
      "   Batch 400 of 481. Elapsed: 0:17:27   \n",
      "   Batch 440 of 481. Elapsed: 0:19:12   \n",
      "   Batch 480 of 481. Elapsed: 0:20:57   \n",
      "\n",
      "   Average training loss: 0.29173104459586363\n",
      "   Training epoch took: 1259.5361368656158\n",
      "\n",
      "Running validation...\n",
      "Accuracy: 0.8148148148148148\n",
      "Validation time: 31.678513765335083\n",
      "\n",
      "###### Epoch 3/4 ########\n",
      "Training\n",
      "   Batch 40 of 481. Elapsed: 0:01:44   \n",
      "   Batch 80 of 481. Elapsed: 0:03:28   \n",
      "   Batch 120 of 481. Elapsed: 0:05:13   \n",
      "   Batch 160 of 481. Elapsed: 0:06:57   \n",
      "   Batch 200 of 481. Elapsed: 0:08:42   \n",
      "   Batch 240 of 481. Elapsed: 0:10:26   \n",
      "   Batch 280 of 481. Elapsed: 0:12:11   \n",
      "   Batch 320 of 481. Elapsed: 0:13:57   \n",
      "   Batch 360 of 481. Elapsed: 0:15:42   \n",
      "   Batch 400 of 481. Elapsed: 0:17:26   \n",
      "   Batch 440 of 481. Elapsed: 0:19:10   \n",
      "   Batch 480 of 481. Elapsed: 0:20:55   \n",
      "\n",
      "   Average training loss: 0.17953349523664514\n",
      "   Training epoch took: 1257.1571881771088\n",
      "\n",
      "Running validation...\n",
      "Accuracy: 0.8217592592592593\n",
      "Validation time: 31.669404983520508\n",
      "\n",
      "###### Epoch 4/4 ########\n",
      "Training\n",
      "   Batch 40 of 481. Elapsed: 0:01:44   \n",
      "   Batch 80 of 481. Elapsed: 0:03:28   \n",
      "   Batch 120 of 481. Elapsed: 0:05:11   \n",
      "   Batch 160 of 481. Elapsed: 0:06:55   \n",
      "   Batch 200 of 481. Elapsed: 0:08:39   \n",
      "   Batch 240 of 481. Elapsed: 0:10:23   \n",
      "   Batch 280 of 481. Elapsed: 0:12:06   \n",
      "   Batch 320 of 481. Elapsed: 0:13:53   \n",
      "   Batch 360 of 481. Elapsed: 0:15:44   \n",
      "   Batch 400 of 481. Elapsed: 0:17:35   \n",
      "   Batch 440 of 481. Elapsed: 0:19:26   \n",
      "   Batch 480 of 481. Elapsed: 0:21:18   \n",
      "\n",
      "   Average training loss: 0.11012249683918675\n",
      "   Training epoch took: 1281.0943267345428\n",
      "\n",
      "Running validation...\n",
      "Accuracy: 0.8310185185185185\n",
      "Validation time: 35.3659131526947\n",
      "------Training complete------\n"
     ]
    }
   ],
   "source": [
    "#Store average loss value for showing in graph later\n",
    "loss_values=[]\n",
    "\n",
    "#For each epoch\n",
    "for epoch_i in range(0,epochs):\n",
    "    print('')\n",
    "    print(f'###### Epoch {epoch_i +1 }/{epochs} ########')\n",
    "    print('Training')\n",
    "    \n",
    "    t0 = time.time() # start timer\n",
    "    total_loss = 0 # reset total loss for each epoch\n",
    "    \n",
    "    classifier.train() # put the model in training mode\n",
    "    \n",
    "    #For each batch of training data\n",
    "    #Actual batch training loop\n",
    "    \n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        \n",
    "        #Provide progress update every 40 batches\n",
    "        if step % 40 == 0 and not step == 0:\n",
    "            elapsed = format_time(time.time()-t0)            \n",
    "            print(f'   Batch {step} of {len(train_dataloader)}. Elapsed: {elapsed}   ')\n",
    "        \n",
    "        #Next step, unpack training batch from dataloader\n",
    "        #Each batch contains 3 tensors: [0] input_ids, [1] attention masks, [2] labels\n",
    "        #Each tensor gets copied to the GPU with .to(device)\n",
    "        \n",
    "        b_input_ids = batch[0].to(device) \n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)        \n",
    "        \n",
    "        #Clear the gradients, otherwise per default for RNN gradients are accumulated, which is not desired here\n",
    "        \n",
    "        classifier.zero_grad()\n",
    "        \n",
    "        #Perform a forward pass, i.e. evaluate model on the training batch\n",
    "        #More info on model function in: https://huggingface.co/transformers/v2.2.0/model_doc/bert.html\n",
    "        \n",
    "        outputs = classifier(b_input_ids,\n",
    "                       token_type_ids = None,\n",
    "                       attention_mask = b_input_mask,\n",
    "                       labels = b_labels) # because we add the labels vector, output will return loss\n",
    "        \n",
    "        loss = outputs[0]\n",
    "        \n",
    "        total_loss += loss.item() # loss is actually a single value in the tensor\n",
    "        \n",
    "        # Perform backward pass to calculate gradients\n",
    "        loss.backward()\n",
    "        \n",
    "        #Gradient clipping for preventing 'exploding gradients'\n",
    "        torch.nn.utils.clip_grad_norm_(classifier.parameters(),1.0)\n",
    "        \n",
    "        #Optimizer makes adjustment to the weights, according to gradients\n",
    "        optimizer.step()\n",
    "        \n",
    "        #Update the learning rate, i.e. apply decay rate\n",
    "        scheduler.step()\n",
    "        \n",
    "    #Average training loss\n",
    "    avg_train_loss = total_loss / len(train_dataloader)\n",
    "    \n",
    "    #Store in list for later plotting\n",
    "    loss_values.append(avg_train_loss)\n",
    "    \n",
    "    print('')\n",
    "    print(f'   Average training loss: {avg_train_loss}')\n",
    "    print(f'   Training epoch took: {time.time()-t0}')\n",
    "          \n",
    "    print('')\n",
    "    print('Running validation...')\n",
    "          \n",
    "    t0 = time.time()\n",
    "    \n",
    "    classifier.eval() #set model to evalutation mode, drop out layers in BERT behave differently\n",
    "          \n",
    "    eval_loss, eval_accuracy = 0,0\n",
    "    nb_eval_steps, nb_eval_examples = 0,0\n",
    "        \n",
    "    #Evaluate for each epoch, i.e. run the model as is on the validation set\n",
    "    for batch in validation_dataloader:\n",
    "        \n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "          \n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "          \n",
    "        with torch.no_grad():\n",
    "              outputs = classifier(b_input_ids,\n",
    "                                   token_type_ids = None,\n",
    "                                   attention_mask = b_input_mask) \n",
    "              # because we add no labels vector, output will return logits, i.e. results\n",
    "        \n",
    "        logits = outputs[0] # logits are the outputs prior to applying the activation function\n",
    "                \n",
    "        #Move logits to CPU\n",
    "        \n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "          \n",
    "        #Calculate the accuracy for this batch of test\n",
    "        tmp_eval_accuracy = flat_accuracy(logits,label_ids)\n",
    "          \n",
    "        eval_accuracy += tmp_eval_accuracy\n",
    "          \n",
    "        nb_eval_steps += 1\n",
    "    \n",
    "    print(f'Accuracy: {eval_accuracy/nb_eval_steps}')\n",
    "    print(f'Validation time: {time.time()-t0}') \n",
    "    \n",
    "print(\"------Training complete------\")\n",
    "          "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plotting average training loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(style='darkgrid',font_scale=1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x432 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.rcParams['figure.figsize'] = (12,6)\n",
    "plt.plot(loss_values,'b-o')\n",
    "plt.title('Average Training Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluating the classifier model on the test set provided by the CoLA dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: Somebody just left - guess who.\n",
      "Tokenized: ['somebody', 'just', 'left', '-', 'guess', 'who', '.']\n",
      "Encoded: [101, 8307, 2074, 2187, 1011, 3984, 2040, 1012, 102]\n"
     ]
    }
   ],
   "source": [
    "# Access data and pass to numpy arrays, same as with training dataset\n",
    "import pandas as pd\n",
    "df = pd.read_csv('./cola_public/raw/out_of_domain_dev.tsv', sep='\\t', header=None, names=['sentence_source','label','label_notes','sentence'])\n",
    "sentences = df.sentence.values\n",
    "labels = df.label.values\n",
    "print(f\"Original: {sentences[0]}\")\n",
    "print(f\"Tokenized: {T_tokenizer.tokenize(sentences[0])}\")\n",
    "print(f\"Encoded: {T_tokenizer.encode(sentences[0])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating input_ids and attention_masks\n",
    "input_ids=[]\n",
    "for sent in sentences:\n",
    "    encoded_sent = T_tokenizer.encode(sent,add_special_tokens=True,max_length=64,pad_to_max_length=True)\n",
    "    input_ids.append(encoded_sent)\n",
    "attention_masks = []\n",
    "for sent in input_ids:\n",
    "    att_mask = [int(token_id > 0) for token_id in sent]\n",
    "    attention_masks.append(att_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert into torch tensors\n",
    "prediction_inputs = torch.tensor(input_ids)\n",
    "prediction_labels = torch.tensor(labels)\n",
    "prediction_masks = torch.tensor(attention_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create dataloader\n",
    "batch_size = 16\n",
    "\n",
    "prediction_data = TensorDataset(prediction_inputs,prediction_masks,prediction_labels)\n",
    "prediction_sampler = RandomSampler(prediction_data)\n",
    "prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting labels for 516 test sentences\n",
      "Prediction time: 0:00:04\n",
      "Prediction time: 0:00:05\n",
      "Prediction time: 0:00:06\n",
      "Prediction time: 0:00:07\n",
      "Prediction time: 0:00:08\n",
      "Prediction time: 0:00:09\n",
      "Prediction time: 0:00:10\n",
      "Prediction time: 0:00:11\n",
      "Prediction time: 0:00:12\n",
      "Prediction time: 0:00:13\n",
      "Prediction time: 0:00:14\n",
      "Prediction time: 0:00:15\n",
      "Prediction time: 0:00:16\n",
      "Prediction time: 0:00:17\n",
      "Prediction time: 0:00:18\n",
      "Prediction time: 0:00:19\n",
      "Prediction time: 0:00:20\n",
      "Prediction time: 0:00:21\n",
      "Prediction time: 0:00:22\n",
      "Prediction time: 0:00:23\n",
      "Prediction time: 0:00:24\n",
      "Prediction time: 0:00:25\n",
      "Prediction time: 0:00:26\n",
      "Prediction time: 0:00:27\n",
      "Prediction time: 0:00:28\n",
      "Prediction time: 0:00:29\n",
      "Prediction time: 0:00:30\n",
      "Prediction time: 0:00:31\n",
      "Prediction time: 0:00:32\n",
      "Prediction time: 0:00:33\n",
      "Prediction time: 0:00:34\n",
      "Prediction time: 0:00:35\n",
      "Prediction time: 0:00:36\n",
      "------Prediction complete------\n"
     ]
    }
   ],
   "source": [
    "#Prediction on dataset\n",
    "\n",
    "print('')\n",
    "print(f'Predicting labels for {len(prediction_inputs)} test sentences')\n",
    "          \n",
    "t0 = time.time()\n",
    "    \n",
    "classifier.eval() #set model to evalutation mode, drop out layers in BERT behave differently\n",
    "          \n",
    "predictions, true_labels = [],[]\n",
    "        \n",
    "#Predict\n",
    "for batch in prediction_dataloader:\n",
    "    \n",
    "    batch = tuple(t.to(device) for t in batch)\n",
    "          \n",
    "    b_input_ids, b_input_mask, b_labels = batch\n",
    "          \n",
    "    with torch.no_grad():\n",
    "        \n",
    "        outputs = classifier(b_input_ids,\n",
    "                             token_type_ids = None,\n",
    "                             attention_mask = b_input_mask) \n",
    "          # because we add no labels vector, output will return logits, i.e. results\n",
    "        \n",
    "    logits = outputs[0] # logits are the outputs prior to applying the activation function\n",
    "                \n",
    "    #Move logits to CPU\n",
    "        \n",
    "    logits = logits.detach().cpu().numpy()\n",
    "    label_ids = b_labels.to('cpu').numpy()\n",
    "          \n",
    "    #Store predictions and true labels\n",
    "    predictions.append(logits)\n",
    "    true_labels.append(label_ids)\n",
    "        \n",
    "    print(f'Prediction time: {format_time(time.time()-t0)}') \n",
    "    \n",
    "print(\"------Prediction complete------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive samples: 354 of 516 (0.686046511627907)\n"
     ]
    }
   ],
   "source": [
    "print(f'Positive samples: {df.label.sum()} of {len(df.label)} ({df.label.sum()/len(df.label)})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating Matthew Corr Coeff for each batch...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\ai_lab\\lib\\site-packages\\sklearn\\metrics\\_classification.py:900: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)\n"
     ]
    }
   ],
   "source": [
    "print('Calculating Matthew Corr Coeff for each batch...')\n",
    "matthews_set = []\n",
    "\n",
    "for i in range(len(true_labels)):\n",
    "    pred_labels_i = np.argmax(predictions[i],axis=1).flatten()\n",
    "    matthews = metrics.matthews_corrcoef(true_labels[i],pred_labels_i)\n",
    "    matthews_set.append(matthews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.3333333333333333,\n",
       " 0.4472135954999579,\n",
       " 0.5606119105813882,\n",
       " 1.0,\n",
       " 0.5606119105813882,\n",
       " 0.8320502943378436,\n",
       " 0.7333333333333333,\n",
       " 0.4622501635210242,\n",
       " 0.4622501635210242,\n",
       " 0.0,\n",
       " 0.8563488385776753,\n",
       " 0.5606119105813882,\n",
       " 0.38297084310253526,\n",
       " 0.42857142857142855,\n",
       " 0.3133397807202561,\n",
       " 0.2219167619785609,\n",
       " 0.40451991747794525,\n",
       " 0.6831300510639732,\n",
       " 0.4622501635210242,\n",
       " 0.7745966692414834,\n",
       " 0.6180700462007377,\n",
       " 0.3133397807202561,\n",
       " 0.5897435897435898,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.7090909090909091,\n",
       " 0.5447047794019222,\n",
       " 0.4472135954999579,\n",
       " 0.15289415743128767,\n",
       " 0.7125253031944253,\n",
       " -0.09759000729485333,\n",
       " 0.5897435897435898,\n",
       " 0.0]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matthews_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Combine predictions and labels into single list\n",
    "flat_predictions = [item for sublist in predictions for item in sublist]\n",
    "flat_predictions = np.argmax(flat_predictions,axis=1).flatten()\n",
    "\n",
    "flat_true_labels = [item for sublist in true_labels for item in sublist]\n",
    "\n",
    "mcc = metrics.matthews_corrcoef(flat_true_labels, flat_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MCC: 0.519\n"
     ]
    }
   ],
   "source": [
    "print('MCC: %.3f' %mcc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
